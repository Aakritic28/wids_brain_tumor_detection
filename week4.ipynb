{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNAKSA4wq+bZV+JKEKKgsor"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"o3Z_0oNLlCkI"}},{"cell_type":"markdown","source":["Implementing SimpleCNN for braintumor detection\n"],"metadata":{"id":"a_JNMT1I-XjG"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oHHNbAK-OYu","executionInfo":{"status":"ok","timestamp":1705731426488,"user_tz":-330,"elapsed":1723016,"user":{"displayName":"Aakriti Chandra","userId":"16304557061618439034"}},"outputId":"5400930c-209f-4120-d929-c62965106c29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Accuracy :  82.35%\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torchvision import datasets, transforms\n","from torchvision.io import read_image\n","from google.colab import drive\n","drive.mount('/content/drive')\n","dataset_path='/content/drive/My Drive/Colab Notebooks/brain_tumor_dataset'\n","\n","transform_training=transforms.Compose([\n","    transforms.Resize((240,240)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFlip(),\n","    transforms.ToTensor(),\n","])\n","\n","dataset=datasets.ImageFolder(root=dataset_path, transform=transform_training)\n","dataset_size=len(dataset)\n","train_size=int(0.8* dataset_size)  #taking 80% data in training\n","test_size=dataset_size -train_size\n","\n","train_dataset, test_dataset= random_split(dataset, [train_size, test_size])\n","\n","batch_size = 32\n","train_loader=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader=DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","\n","#CNN model\n","class CNN(nn.Module):\n","  def __init__ (self):\n","    super(CNN, self).__init__()\n","    self.conv1=nn.Conv2d(3,32,kernel_size=3, padding=1)\n","    self.activation=nn.ReLU()\n","    self.batch_norm=nn.BatchNorm2d(32)\n","    self.maxpool2d=nn.MaxPool2d(kernel_size=2, stride= 4)\n","    self.maxpool2d_1=nn.MaxPool2d(kernel_size=2, stride=2)\n","    self.maxpool2d_2=nn.MaxPool2d(kernel_size=2, stride=2)\n","    self.flatten=nn.Flatten()\n","    self.dense=nn.Linear(32*15*15, 128)\n","    self.dropout=nn.Dropout(0.5)\n","    self.dense_1=nn.Linear(128,2)\n","    self.softmax=nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","    x=self.conv1(x)\n","    x=self.activation(x)\n","    x=self.batch_norm(x)\n","    x=self.maxpool2d(x)\n","    x=self.maxpool2d_1(x)\n","    x=self.maxpool2d_2(x)\n","    x=self.flatten(x)\n","    x=self.dense(x)\n","    x=self.dropout(x)\n","    x=self.dense_1(x)\n","    x=self.softmax(x)\n","    return x\n","\n","model=CNN()\n","loss_check=nn.CrossEntropyLoss()\n","optimizer=optim.Adam(model.parameters(), lr=0.001)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","\n","# num of iterations for training\n","iters=10\n","for epoch in range(100):\n","  model.train()\n","  for batch, labels in train_loader:\n","    optimizer.zero_grad()\n","    outputs=model(batch)\n","    loss=loss_check(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","model.eval()\n","correct_predictions=0\n","total_samples=0\n","with torch.no_grad():\n","  for batch, labels in test_loader:\n","    outputs= model(batch)\n","    _, predicted=torch.max(outputs, 1)\n","    total_samples+=labels.size(0)\n","    correct_predictions+=(predicted==labels).sum().item()\n","\n","accuracy=correct_predictions/total_samples\n","print(f'Accuracy : {accuracy*100 : .2f}%')\n","\n"]}]}